# Next-Video-Frame-Prediction-using-LSTM-PREDRNN--TRANSFORMER
# UCF101 Action Recognition and Frame Prediction

## Overview
This project focuses on video frame prediction using the UCF101 dataset, which features human activity videos. It implements and compares three deep learning models: **ConvLSTM**, **PredRNN**, and **Transformer-based architectures**. The models aim to predict future video frames based on input sequences, enabling applications in action recognition, surveillance, and video synthesis.

---

## Dataset
The project uses the [UCF101 Action Recognition Dataset](https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition/data). This dataset contains videos of human activities, which are preprocessed into sequences for model training.

---

## Features
1. **Video Frame Prediction Models**
   - **ConvLSTM**: Combines convolutional and LSTM layers for spatial-temporal modeling.
   - **PredRNN**: Enhances temporal modeling with spatiotemporal memory.
   - **Transformer-based**: Captures long-term dependencies using self-attention mechanisms.

2. **Evaluation Metrics**
   - **Mean Squared Error (MSE)**: Quantifies pixel-level differences.
   - **Structural Similarity Index (SSIM)**: Evaluates perceptual quality.

## Contributions
Duaa Fatima, Nayyera Wasim, Syeda Mahum Raza


